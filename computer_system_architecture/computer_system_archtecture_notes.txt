计算机系统架构

评分
    期末考试 40~50%
    实验 20~30%
    报告 5~15%
    手写作业 10~20%

计算机系统
   1. CPU
   2. 存储
   3. 网路, 又分为机器内, 机器间的

概念
    Flynn 分类:
        SISD    普通 CPU
        SIMD    向量 CPU
        MISD    只是一个概念
        MIMD    多核 CPU
    透明性:
        从高层来看, 底层本来存在的属性却好像看不到一样
    系列机
        同一厂家 (事实上不一定), 具有相同系统结构, 只是组成和实现不同

Amadahl 定律
    系统加速比 = 原执行时间 / 改进后执行时间
    现在改进系统
        0 < Fe < 1 的部分得到了改进, 这一部分加速比是 Se
        那么系统的加速比是  1 / (1 - Fe + Fe/Se)
    重要启示:
        改进 Fe 的部分, 无论改进多好, 加速比不会超过
            1 / (1 - Fe)

CPU 性能
    CPU 执行一个程序的时间
        CPU时间 = CI * CPI * CYCTIME
        CI  执行的指令条数
        CPI 平均一条指令执行多少周期
        CYCTIME 时间周期长度

------------------------------------------------------------------------------
指令系统的结构
    栈     操作数在栈上
    累加器 操作数在累加器里
    寄存器 操作数在寄存器里
        RegReg / RegMem 寄存器: 是否允许除了 load store 外的指令访问内存
        2 操作数 / 3 操作数
        现代主流是寄存器

寻址方式
    寻址指的是去哪里寻找操作数, 而不是狭义的访问内存
        例如, 寄存器寻址, 立即数寻址, 偏移寻址...
编址方式
    如何将不同宽度的信息存放到存储器中
    现代方案: 信息多宽, 就按多对齐
        访问速度和存储器利用率的折中
程序定位
    * linker 定位: 程序自身就确定了位置信息
    * static loader 定位: 程序在加载时定位
    * dynamic loader 定位: 执行到了才定位

指令设计
    跳转指令三种风格: 条件码 CC; setlt, seteq; brlt, breq;
        跳转地址风格: PCrel; ABS; ...

指令格式优化
    高效地缩短指令编码
操作码:
    * 定长编码
    * Huffman 编码
    * 拓展操作码
        类似 Huffman 编码, 但是最后要求编码长度是确定的几个数之一
        一般采用等长拓展码
        15/15/15拓展法
            操作码 4 位: 15 种  0000 -> 1110
            操作码 8 位: 1 * 15 种  1111 0000 -> 1111 1110
            操作码 12 位: 1 * 1 * 15 种 ...
        8/64/512拓展法
            操作码 4 位: 8 种 0000 -> 0111
            操作码 8 位: 8 * 8 种 1000 0000 -> 0111 1111
    信息冗余量:
        i是指令, 某种编码 l 的信息冗余量 (sum pi li) / (sum pi log 1/pi)
        Huffman 编码冗余量不是 0, 因为编码长度必须是整数
操作数
    变长 定长 (RISC 一般使用这种) 混合

CISC 和 RISC, 以及 VLIW
    CISC:
        古代 ROM 比 RAM 便宜快速, 使用 ROM 来存储微程序
        加上古代 CPU 很慢, 所以产生了 CISC 的想法
        古代人甚至有 "高级语言计算机" 的想法 (传说中的 LISP Machine?)
        优化重点: IC * CPI * TCYC 中减少 IC
    RISC:
        20 世纪后期, CPU 变快, 缓存出现
        原来 ROM 的空间拿来做缓存, 指令规整有利于流水实现
        优化重点: IC * CPI * TCYC 中减少 CPI, 可能的话 TCYC
    VLIW:
        引入超常指令, 试图在 ISA 级别显式化 ILP (Instr. Level Parallelism)
        21 世纪初, 现在没人用: 编译器太难写

------------------------------------------------------------------------------
Cache 系统

局部性
    是程序本身的特性
    时间局部性  被访问的数据在一段时间内很可能再次被访问
    空间局部性  被访问的数据周围的数据很可能也需要被访问

cache 衡量指标
    不能单纯使用缺失率:
        如果 cache 策略极其复杂造成命中时间很高,
        那么缺失率降低并非好事
    cache 平均访问时间 = 命中时间 + 缺失率 * 缺失损失
        缺失损失包括: 从更远端的存储取得数据, 保存到这里, 将数据传输给 CPU

块映射规则
    块: 从主存调入 cache 的最小数据单位
    内存地址 -> tag || index || offset
    全相联:
        (矩阵看法) 1 x n;
        |index| = 0
        冲突概率最低 -> TLB
    直接映射:
        n x 1;
        |tag| = 0
        实现最简单
    组相联:
        m x n
        主流;
        相联度是 n, n 等于 1 就是直接映射

查找算法
    注意 index 在低位而 tag 在高位
    * 顺序查找
    * 并行查找: 硬件实现
        比较 tag 和读取数据并行执行

替换策略
    cache 的缺失
        * compulsory miss: 启动缺失.
            cache 无限大时, 所有的 miss 都是 compulsory miss.
        * capacity miss: 工作集大于 cache 容量.
            全相联 cache 相对无限大 cache 增加的都是 capacity miss.
        * conflict miss: 因为冲突造成的 miss.
            相对于等大小的全相联增加的 miss 全是 conflict miss
    策略
        选择 victim 都是从同一组 (i.e. 同一行) 中选择的.
        优先选择 invalid 块 (未装载数据的块) 而非替换其他有数据的块
        * LFU   每个块有一个访问计数, 替换访问计数最少的块
        * LRU   替换上次访问时间最老的块
            缺失率相对很低
            硬件实现:
                栈方法.  访问块, 将块放到栈顶.  替换块, 选择栈底块.
                比较对法. cache 块每一对 <A, B> 维护 T_AB
                    表示 A 最近访问是否比 B 最近访问要晚
            太麻烦, 采用更简单的 PLRU (Pseudo LRU)
        * BT
            维护一颗二叉决策树, 叶子节点是块, 决策是选择那个块当 victim
            每次访问某块, 找到从根到这块的路径, 路径上的边全部设成 0,
            路径边的兄弟边设成 1
        * NRU
            每个块维护一个 NRU 位, 装载和访问该块使得 NRU 变成 0
            选择 NRU 为 1 的替换. 如果 NRU 都是 0, 那么全部设成 1
            然后随机替换.
        * NMRU
            保存一个指针记录最近访问的是哪一个块
            替换时随机选择一个不是最近访问的块就好

写策略
    写指令一定是在确认命中的时候再执行
    * write through 写直达法
        写指令对 cache 和主存都要写
        写缓冲区: 减少写主存造成的停顿
            解决 RAW: R 之前清空写缓冲区; 或者读主存 / cache 前检查写缓冲区
    * write back 写回法
        写指令只写 cache 并且设置 modify 位, 在换出 cache 块时候再写回主存
    - 写分配
        写缺失时候, 先换入要写的块, 再写
    - 写绕过
        写缺失时候直接写到内存里

cache 性能分析
    如前, 平均访存时间 = 命中时间 + 缺失率 * 缺失损失时间
    存储器停顿时钟周期数 = 访存次数 * 缺失率 * 缺失损失周期数
    TCPU = IC * (CPI_exec + AVGMA * MR * MP) * TCYC
        CPI_exec    不考虑停顿, 指令的执行时间
        AVGMA       平均每条指令访存次数
        MR          缺失率
        MP          缺失损失

------------------------------------------------------------------------------
Cache 性能改进

降低缺失率
    许多降低缺失率的方法会增加缺失损失或者命中时间
    * 增加块大小
        超过临界点之后块越大缺失率越大
        减少了 compulsory, 但是可能增加 conflict
        增加缺失损失
    * 增加 cache 容量: 增加成本
        可能增加命中时间
    * 提高相联度
        经验: 容量 N 的直接映像 cache 和容量 N/2 的两路组相联 cache
            缺失率差不多
        提高命中时间
        相联度超过临界点后, 提升不大
    * 伪相联 cache
        直接映射的空间分为上下两部分, 访问先去上面查找,
        找到则和直接映像一样 否则再去下面一样的位置查找,
        这时发生的命中称为 "伪命中"
        多种命中时间不能用于 L1 cache
    * victim cache
        换出的 cache 先进到该级 cache 和下级 cache 之间一个很小的
            全相联 cache, 一杯重用.
        能够有效减少冲突缺失
    * 硬件 prefetch
        降低 compulsory 缺失
        不能影响对正常缺失的处理
    * 指令 prefetch
        必须是非阻塞的, 并且得到的好处必须超过这条预取指令的代价
        可以使得取数据和执行指令并行化
    * 编译器优化
        代码和数据重组:
            代码基本块入口和块对齐
            对于很可能发生的跳转, 将跳转目标直接放到跳转之后
            数组合并
            内外循环交换
            分块
        矩阵乘法的例子

降低缺失损失
    * 读缺失优先于写
        写缓冲区的引入
        - 读缺失前等待写缓冲区排空
        - 读缺失时抓去数据还要去写缓冲区看看
    * 写缓冲合并
        写透法中, 缓存写内存某地址时,
        如果检查到写缓冲区里面也要写那个地址,
        则直接和写缓冲区中的写操作合并即可
    * 请求字处理
        CPU 需要的只是一个字而非整个块的内容,
        从缓存调取数据时直接调取这个字
    * 非阻塞 cache
        cache 缺失时 CPU 不暂停
        一般 "一次缺失下命中" (不需要多次缺失情况下还继续执行) 即可
    * 采用多级 cache
        平均访存时间 = HT_L1 + MR_L1 * (HT_L2 + MR_L2 * MP_L2)
        MR_L2 是 L2 的局部缺失率 (到达 L2 的请求中多少缺失)
        一般使用全局缺失率 (CPU 的请求有多少穿过了 L1 和 L2),
            为了体现 L1 和 L2 的协作
            -> L2 局部缺失率和一样大小的 L1 缺失率差不多
        L2 一般很大, 而且可以采用较高相联度 (命中时间不是那么重要了)
            cache 块更大

降低命中时间
    * 使用容量小, 结构简单的 cache
    * cache 地址使用虚拟地址 (传统使用物理地址)
        并行进行 cache 访问和地址转换
        需要考虑进程切换时的 cache 清空, 或者加上 pid 标志
        > 可以直接使用 page offset (这个直接从虚地址拿到) 作为 index
            之后读取 tag 和 vp 翻译可以并行执行
            但是限制了 cache 的大小: 矩阵视角的一列 cache 不能超过一页
    * 流水化 cache 访问
        并不减少访问延迟, 但是可以提升访问带宽和时钟频率
    * trace cache
        存储 CPU 执行的动态指令序列
            -> 主存存储的是静态指令序列, 不能反映跳转等情况
        复杂, 相同指令序列可能被重复存放
        提高 I cache 的利用率

------------------------------------------------------------------------------

并行主存系统
    一个访存周期访问多个字

单体多字存储器
    一次访问主存可以返回 m w 长度的数据, w 是一个字的长度
    实现简单
    容易冲突: 取到的 m w 的数据可能有相当一部分是无效的
    只写一个字就会变得复杂

多体交叉存储器
    由多个独立的单体单字存储器构成
    地址编制:
        高位交叉:
            物理地址 = 存储器号 || 存储器内字号 || "00"
            如图, 每个方格是一个字, 里面数字是地址

            体号\体内地址   0      1      2      3
                        +------+------+------+------+
                        |      |      |      |      |
                  MEM#0 |  0   |  1   |  2   |  3   |
                        |      |      |      |      |
                        +------+------+------+------+
                        |      |      |      |      |
                  MEM#1 |  4   |  5   |  6   |  7   |
                        |      |      |      |      |
                        +------+------+------+------+
                        |      |      |      |      |
                  MEM#2 |  8   |  9   |  10  |  11  |
                        |      |      |      |      |
                        +------+------+------+------+
                        |      |      |      |      |
                  MEM#3 |  12  |  13  |  14  |  15  |
                        |      |      |      |      |
                        +------+------+------+------+
        低位交叉
            如图, 相对高位来说提高访问速度更好 (冲突更少)
                 MEM#0  MEM#1  MEM#2  MEM#3
                +------+------+------+------+
                |      |      |      |      |
                |  0   |  1   |  2   |  3   |
                |      |      |      |      |
                +------+------+------+------+
                |      |      |      |      |
                |  4   |  5   |  6   |  7   |
                |      |      |      |      |
                +------+------+------+------+
                |      |      |      |      |
                |  8   |  9   |  10  |  11  |
                |      |      |      |      |
                +------+------+------+------+
                |      |      |      |      |
                |  12  |  13  |  14  |  15  |
                |      |      |      |      |
                +------+------+------+------+

实际带宽分析
    取指令的带宽
        如果指令都是顺序, 没有分支, 那么可以达到最大带宽 m w
        申请序列: CPU 发出的一串连续的取值序列, 每条取值都去不同存储体
        EXPECTANCE[申请序列长度] 接近存储体的数目 m, 则效率越高
        设转移概率是 lambda, 则 EXPECTANCE = (1-(1-lambda)^m) / lambda
    数据的带宽
        数据的顺序性比指令差
        一般取 m <= 8

存储体冲突
    两个请求访问同一个存储体
    * 增加存储体数目
    * 软件方法: 合理安排数据
        让数组大小不是 2 的幂
    * 硬件方法: 存储体数量是素数
        取模交叉: 体内地址 = 地址 % 体内地址大小
                 MEM#0  MEM#1  MEM#2
                +------+------+------+
                |      |      |      |
                |  0   |  4   |  8   |
                |      |      |      |
                +------+------+------+
                |      |      |      |
                |  9   |  1   |  5   |
                |      |      |      |
                +------+------+------+
                |      |      |      |
                |  6   |  10  |  2   |
                |      |      |      |
                +------+------+------+
                |      |      |      |
                |  3   |  7   |  11  |
                |      |      |      |
                +------+------+------+

无冲突访问例子
    一维数组:
        如果访问 stride 是存储体数目的因子, 那么效率就达不到设计带宽
        解决: 素数个存储体
    二维数组:
        要求按行, 列, 对角线和反对角线都能达到设计带宽
            事实上就是一维数组, 在要求的 stride 下达到设计带宽
        n x n 方案: 存储体数目 m > n 为素数.
        令 m = 2^{2P} + 1; d = 2^P, 那么可以无冲突存放
        e.g. n = 4, m = 5, d = 2
             MEM#0  MEM#1  MEM#2  MEM#3  MEM#4
            +------+------+------+------+------+
            |      |      |      |      |      |
            |  00  |  01  |  02  |  03  |      |
            |      |      |      |      |      |
            +------+------+------+------+------+
            |      |      |      |      |      |
            |  13  |      |  10  |  11  |  12  |
            |      |      |      |      |      |
            +------+------+------+------+------+
            |      |      |      |      |      |
            |  21  |  22  |  23  |      |  20  |
            |      |      |      |      |      |
            +------+------+------+------+------+
            |      |      |      |      |      |
            |      |  30  |  31  |  32  |  33  |
            |      |      |      |      |      |
            +------+------+------+------+------+
        虽然实现简单 (
            行数就是体内地址, 列数+d*行数 mod 体数 就是体号)
        但是浪费空间
    二维数组
        行, 列, 对角, 反对角均达到设计带宽
        n x n, n = 2^{2P}
         MEM#0  MEM#1  MEM#2  MEM#3
        +------+------+------+------+
        |      |      |      |      |
        |  0   |  8   |  12  |  4   |
        |      |      |      |      |
        +------+------+------+------+
        |      |      |      |      |
        |  9   |  1   |  5   |  13  |
        |      |      |      |      |
        +------+------+------+------+
        |      |      |      |      |
        |  14  |  6   |  2   |  10  |
        |      |      |      |      |
        +------+------+------+------+
        |      |      |      |      |
        |  7   |  15  |  11  |  3   |
        |      |      |      |      |
        +------+------+------+------+
        问题就是, 地址到体号, 体内地址的映射非常麻烦

------------------------------------------------------------------------------
流水线技术

基本概念
    通过时间
        第一个任务进入流水线到其输出结果的时间
    排空时间
        最后一个任务进入流水线到其输出结果的时间
    时空图
        [tsg.png]
    线性流水线 / 非线性流水线: 是否有反馈路径
    顺序流水线 / 乱序流水线: 任务流出顺序是否和流入相同
    瓶颈段
        耗时最长的流水线阶段

量化分析
    吞吐率
        单位时间内流水线完成的任务
        最大吞吐率: 假设一个稳定的任务流
        实际吞吐率: 有限的 n 个任务.
            假设 k 段线性流水线, 每段相等为 DT,
            实际吞吐率 = n / ((n+k-1) DT)
            (每段不等的情况类似)
        提高吞吐率:
            细化瓶颈段
            多个瓶颈原件: 允许多个任务进入瓶颈段
    加速比
        S = 不使用流水线完成一批任务时间 / 使用流水线完成一批任务时间
        最大加速比  k
        实际加速比  nk / (n+k-1)
    效率
        等于 n / (n+k-1) 等语 实际加速比 / 最大加速比

现实问题
    瓶颈段问题
        瓶颈细分
    流水寄存器延迟
        流水寄存器需要建立和保持时间
    时钟偏移
        时钟到达各个流水寄存器会有偏差
    流水线的深度最终会受制于寄存器延迟以及时钟偏差等无法优化的外部因素

非线性流水线的调度
    非线性流水图不能唯一地表示工作流程
    -> 需要流水线预约表: 具体说明工作在什么时刻占用什么部件

    无冲突调度方法
        1. 对于每一行, 计算任意两个 X 的距离, 所有行任意 X 之间距离构成
            "禁止启动集合"
            其意义就是, 如果 T1 时刻有一个任务开始执行, 之后 T2 > T1 时刻
            另外一个任务开始执行, 那么
                这两个任务冲突 <=> T2-T1 在禁止启动集合中
            另外设禁止启动集合中最大元素为 m, 两个任务相隔超过 m 显然不会冲突
        2. 构建状态图, 最开始有一个结点就是 禁止启动集合,
            对于每一个结点 U = {u1, u2...},
            对于每一个 1..m+1 之间不在该结点集合中的数 t,
            都有一条边, 从该集合出发, 边上注解是这个数, 目标是
            V = ({ui - t} I Z^+) U 禁止启动集合
        概念
            冲突向量
                ppt 上的概念, 事实上是画蛇添足的无用概念.
                对每一个结点 U, 冲突响亮就是 m 位的 01 向量,
                如果 i 在这个结点的集合里, 那向量的 i 位为 1.
            最小启动循环
                在如上的图的所有简单环 (一个结点最多经过一次) 中, 环上边长平均最少
            恒定循环
                如上构建的图中的自环 (保证存在, 禁止启动集合 -m+1-> 禁止启动集合)

    最优调度和预留算法
        最高效利用流水线部件
        1. 寻找预约表中一行最多几个 X, 设为 P 个
        2. 预约表中, 和第一个 X 距离为 P 倍数都空出来
            通过将 X 右移一个格子 (参见课件)
            硬件实现上增加一个 FF 触发器即可
        3. 选择恒定循环 (P)
        容易证明, 这样设计, 原来 X 最多的那一行在稳定之后每个周期都有事情做,
            也就达到了最高效率

流水线相关
    他的五级流水: IF ID EX MEM WB
        跳转指令在 EX 之后才会计算出地址修改 PC
    数据相关 (真数据相关)
        直接或者间接地, 后面的指令需要前面的指令的结果
        俗称: RAW

    名称相关 (假数据相关)
        两条指令使用了相同的名称但是没有实际上的数据依赖
        名称这里可以指寄存器, 或者存储器
        愚蠢的名称
            反相关: 不引起冲突的 WAR;
            输出相关: 不引起冲突的 WAW
        解决: 换名 (aliasing)
            可以硬件实现, 可以编译器实现
    控制相关
        分支指令引起的控制流的相关

冲突
    结构冲突
        硬件资源的限制
        特别是: IF 和 MEM 的对于单口 RAM 的访存冲突
        方法:   暂停
                独立的 I-RAM 和 D-RAM
        数据流图
            IF  ID  EX  MEM WB
                IF  ID  EX  MEM WB
                    IF  ID  EX  MEM WB
                        *S* IF  ID  EX ...
    数据冲突
        WAW, RAW: 五段流水线不会
        解决:
            硬件转发 (load 可能需要再暂停一周期)
            编译器指令调度
    控制冲突
        解决:
            流水线排空
            提前计算转移与否, 之后
                分支预测
                    在五级流水线中, 不可能预测 taken, 只能使用 never taken
                delay slot      愚蠢的设计
                    其中指令可以 nop, 也可以编译器调度
                分支取消
                    把 taken 后第一条指令放到 delay slot 里
                    如果没有 taken, 再 flush 掉

------------------------------------------------------------------------------
动态流水线算法

指令级并行 ILP
    包括流水线, 多个功能部件, 多发射等方法
    可以编译器做, 也可以硬件来做.

硬件动态流水线调度
    动态地检测流水线相关和冲突, 乱序执行
    基本概念
        流出 IS (issue) 和读码 (RO, read operand)
            五级流水线的 ID 段做的两件事情, 这里我们分开
            IS 就是检测是否有结构冲突
            RO 就是读取操作数
    乱序执行带来的问题是, 出现了新的冲突 WAW WAR, 以及精确异常非常困难

记分牌算法
    CDC 6600 首先使用
    顺序发射, 乱序执行, 乱序完成
    四个阶段
        IS: 如果 1. 功能单元可用; 2. 不会产生 WAW 冲突
            那么发射当前指令
        RO: 如果不会产生 RAW 冲突, 那么那么读取当前指令的操作数
        EX: 在某个功能单元中执行当前指令, 可能花费多个周期
        WB: 如果不会产生 WAR 冲突, 那么将结果写到寄存器, 完成操作
    特点
        不考虑跳转指令, 只考虑基本块中的调度
        WAR 通过等待 (不是暂停) 解决
        WAW 直接是不发射

Tomasulo算法
    1960年代提出, 现代广泛使用, intel x86-64 使用的是 tomasulo 的变种

    组成部分:
        RS (reservation unit):
            每个功能单元都对应一个 RS, 其中保存使用这个功能单元的指令的信息
            事实上 Tomasulo 中不区分 RS 和 FU
                busy:   是否忙
                op:     做什么操作
                Qi,Qj:  源操作数地址. 只可能是另外的 RS,
                        表示源操作数是那个功能单元的计算结果
                Vi,Vj:  源操作数的值.
            Qx 和 Vx 中只有一个有效, 取决于源操作数是否已经就绪 (被计算出来了)
        load buffer 和 swre buffer:
            也是一种特殊的 RS.
        寄存器状态:
            每一个寄存器有一个状态,
            记录哪一个 RS 希望写这个寄存器
            当然也可能是没有 RS 准备写这个寄存器
        CDB (common data bus):
            每个功能单元完成计算之后, 在 CDB 上发送广播
                <功能单元号, 计算结果值>
            寄存器监听, RS 也监听.

    状态
        只有三个状态
        IS: 如果 FU 空闲 (没有结构冲突), 那么就发送指令给 FU.
            具体来说, 需要从指令中抽取操作类型,
            以及从寄存器状态中得到源操作数地址
            一条指令的发射会导致 RS 的变化, 以及目的寄存器的寄存器状态的变化
        EX: 当两个操作数都就绪, 就可以开始执行, 否则监听 CDB.
            任何一个操作数就绪时, 先取得其数据到 Qx 中, Vx 清空.
        WB: 计算完成后, 在 CDB 广播 <RS, 计算结果值>

动态分支预测
    分支预测评价
        * 预测命中率
        * 命中开销, 不命中损失
    BHT (branch hiswry table)
        记录最近若干次的分支情况 (taken? not taken?)
        2 位的 BHT 基本能够很好地预测:
            只有在 BHT 预测错误连续 2 次的时候才改变预测策略
        如果判断分支是否 taken 的时间不是很大, 如五级流水,
            那么 BHT 没有用
    BTB (branch target table)
        对于 taken 的分支指令, 保存如下信息到 BTB
            1. 指令的地址
            2. 跳转到的地址
        之后每次分支, 先去 BTB 检查有无当前指令
        如果有, 预测当前指令 taken, 跳转到目标地址
        否则预测 not taken
        当然无论预测如何, 都还是要实际计算执行 branch 指令的.
        > 还可以把分支后的若干指令放到 BTB 中

预测执行
    预测分支指令的结果, 然后继续执行, 但是执行结果写到 ROB (Reorder buffer) 中,
        仅当分支指令的预测结果确认是对的时候, 再提交这些 ROB 中结果
    特点
        能够给 tomasulo 框架实现精确异常
        乱序执行, 顺序确认
    tomasulo 集成预测执行
        IS  只有在有空闲 RS 和 ROB 的时候才发射
        EX  ...
        WB  CDB 上广播的消息是 <ROB号, 数据>
        也就是说, 原来用于寄存器重命名的数据源 RS
            现在变成了 ROB
        确认
            每次检查 ROB 头部的指令

------------------------------------------------------------------------------
互联与网络
    应用: 通常是超级计算机, 计算机集群中有效通信 (这里通信的要求相对非常高)

静态网络
    网络连接在程序执行过程中不会改变
    不直接相连的结点通过中间结点路由
    概念
        距离: 指的是边距离
        直径: 结点间距离的最大值
        等分宽度: 将网络分成相等大小的两半最小需要除去多少边
    例子
        线性阵列
            注意线性阵列不同于总线. 不可能在一个时刻使用总线的两个部分.
        环
            "链接" 就是完全图
        树形, 星型, ...

动态网络
    网络有开关可控制 i.e. 有源
    置换:
        恒等置换 I
        方体函数 f(X) = f(X xor 2^k), 某一位取反
        洗牌函数 f(X) = f(X <<< 1)
        蝶式函数 f(X) = f(reversed X)
        PM2i函数 f(X, +i) = X + 2^i; f(X, -i) = X - 2^i
        可以使用轮换表示置换
    开关:
        2^N x 2^N 开关的一个状态是
            2^N -> 2^N 的函数 (不要求双射), f(i) 是输出 i 对应那个输入
            当 N=1, 有 "直通", "交叉", "上播", "下播"
        开关就是可以改变其状态
            多个状态可以是冲突的, 称为阻塞网络
    omega 网
        ISC: 从输入到第 0 级, 从第 i 到 i+1 级 是洗牌函数
            从最后一级到输出就是恒等变换
        每个开关可以独立控制
        三级, 2 x 2 开关, 构建 8 x 8 网络
            考虑 o = omega(i), i in 0..7
            i -> |<<<1| -> xor a -> |<<<1| -> xor b -> |<<<1| -> xor c -> o
            其中 a,b,c in {0,1}.
            显然如果 i = i2i1i0, 那么 o = (i2^a, i1^b, i0^c)
        两级, 4 x 4 网络, 构建 16 x 16 网络
            i -> |<<<2| -> xor a -> |<<<2| -> xor b -> o
            其中 a,b in {0,1,2,3}
            显然如果 i = i32i10, 那么 o = (i32^a, i10^b)
    碟状网络
        ISC: 从第 i 到 i+1 级是碟状函数.
            从输入到第 0 级, 最后一级到输出是恒等
        如两级, 8 x 8 开关构建 64 x 64 网络
            i -> xor A -> |<<<4| -> xor B -> o

网络通信
    消息和包
        消息: 逻辑上完整的信息单元
        包: 和协议相关
        片: 通常就是指一个字节
    传输延时公式
        T = TS + TN + TB
        TS: 建立时间. 如从主存拷贝到网络上, 正确性验证等花费的时间
            包含在源的和在目的点的
        TN: 网络传输时间.
            TN = TP * D + L/B
                TP: 消息在他经过的路径上每段 (each hop) 的平均时间
                D: 源到目的的距离
                L/B: 消息长度 / 网络带宽
        TB: 其他时间
    路由算法
        存储转发
            每个中间结点完整地接受包之后, 寻找包的下一跳并且发送
            TN = (L/B)*D + L/B
            [sf.png]
        虚拟直通
            每个中间结点完整地接受包头之后,
            寻找包的下一跳并且连续发送接受到的该包内容
            TN = (LH/B)*D + L/B,    LH 是包头长度
        电路开关
            传递包之前, 先建立预留一条源到目的的物理通道
            TN = (LC/B)*D + L/B
            注意这时传输过程中物理链路不能共享
            [cs.png]
        wormhole
            消息最初是一个头, 包含消息的路由信息, 以及最后有一个终结符号
            之后一个包一个包, 每个包占用一个结点, 向前 "蠕动"
            [wh.png]
    死锁: 可以通过增加虚拟通道来避免死锁
        虚拟通道包含两段的缓冲区, 以及和其他虚拟通道复用的物理链路
    包冲突
        两个入境包请求同一个缓冲区, 或者同一个输出通道
        * 缓冲
        * 阻塞
        * 丢包重传
        * 绕道
    寻径
        考虑结点是 0..2^N-1 的超立方体网络.
            确定性寻径就是, 每一次走一步, 走到结点有多一位和目标结点那位相同
            (sn-1...s1s0) -> (sn-1...s1d0) -> (sn-1...d1d0) -> ... ->
            (dn-1...d1d0)
        自适应寻径
            避免死锁
            网格网络中使用虚拟通道

------------------------------------------------------------------------------
向量处理机
    向量处理机主要为了解决高性能数值计算
    向量: 一组有序的相同类型的元素

标向量平衡
   向量处理机也能处理标量.
   向量平衡点是当处理机标量部件和向量部件都充分被使用时,
   程序中向量代码占比.

向量的表示
    等间距表示 address:stride:length
    位移量表示 base:baselength:offset
        有效长度是 baselength - offset
        起始地址是 base + offset

处理方式
    横向 d = a*b + c, 依次计算 d1, d2... 不适合向量处理机
    纵向 先计算 a*b, 然后加 c 得到 d, 适合向量处理机
    分组 将计算分组, 每组内部纵向, 各组是横向的
        适合向量处理机, 并且向量长度不受限于寄存器长度

处理机架构
    内存内存架构
        源操作数向量和结果向量都在内存中
        对于存储器带宽要求很高, 通常采用并行主存
    寄存器寄存器架构
        分组处理长向量
    注意处理机仍然是标量流水化的, 一次只能处理一个元素的计算

性能调优
    * 多个功能部件, 减少结构冲突
    * 链接技术
        后面指令使用前面的结果,
        而且前面向量在后面指令之后就没有再被使用 (Use-Def 的使用)
        -> 不用写到寄存器中转, 直接链接输出到输入即可
        课上的 cray-1 例子中,
            对向量中每个元素,
            从内存到读取 FU, 从 FU 到输出寄存器,
            从寄存器到 FU 都需要一个周期
    * 分段开采技术
        硬件和系统软件完成对向量的分段处理
    * 使用多处理机

性能评价
    * 单条指令的时间
        T = TS + TE + (n-1) TCYC
        TS  流水线建立时间
        TE  流水线通过时间
    * 一组指令的时间
        指令编队: 链接和没有链接的情况似乎有不同,
            似乎是没链接要求无数据结构冲突, 有链接要求相邻的连接起来
            没有清楚的含义, 课件和书均语焉不详狗屁不通, 照着例子画瓢即可.
        T = ceil(n / L) * (T_loop + T_start) + mn
            n 是向量元素个数
            L 是寄存器中能保存多少个元素,
            m 是编队个数
            T_loop 为分段开采带来的时间开销
            T_start = sum T_start_i,
                后者是每个编队的启动时间,
                无链接时等于编队中指令最大启动时间
                有链接时等于编队中指令启动时间之和
    * 最大性能 R_inf
        向量长度趋于无穷大时, 处理机的性能
        R_inf = lim n->inf 浮点运算次数 / 运算时间
        单位是 flops, 通常为 Mflops, 现在计算机可以达到 Gflops, Tflops ...
        运算次数指的是对于特定的指令序列
        运算时间可以等于 T * TCYC, T 是上文计算出来的
    * 半性能长度 n_0.5
        处理剂性能达到 0.5 R_inf 时所需的向量长度
        参见例子
    * 长度临界值 n_v
        向量处理方式大于标量处理方式时, 最小的向量长度

------------------------------------------------------------------------------
阵列处理机
    基本架构
        控制单元 CU 控制多个执行单元 PE
        CU 负责指令处理, 然后译碼, 翻译之后发送 PE 能够理解的指令给 PE
    * 存储器分布式的
        每个 PE 有私有的存储
        表量指令由 CU 执行, 向量指令通过广播总线广播给 PE 中, 每个 PE
        并行地执行这一条向量指令
    * 存储器共享的
        需要是多体并行存储, 并且存储器的体数通常大于 PE 的个数
    一般单个阵列处理机是不够的, 还需要一台前台机器来完成 I/O 管理, 操作系统等.
        阵列机只负责数值计算

------------------------------------------------------------------------------
多线程和多处理机

------------------------------------------------------------------------------
多核 cache 一致性

一致性协议
    写无效和写更新: 写共有数据的时候是让其他副本无效还是更新
    使用监听, 目录还是令牌协议
    状态是简单状态还是 MESI 等状态

总线监听
    局部 cache 的变化会广播给所有 CPU 以及 cache
    scalability 不好
    状态
        每个 cache 块都有一个状态,
        有一个控制单元负责根据内外部时间的发生来改变这个状态
        以后叙述的状态, 以及转移都是针对这一个块而非整个 cache 的
        INV
            cache 块中还没有内容
        SHR
            cache 块中有内容, 但是主存, 以及其他 cache 中的副本一样
        MOD
            cache 块中内容和主存不一样, 其他 cache 块中不可能有副本
    事件
        事实上, 如下事件都有一个相关的地址作为数据.
        我们在某个块中, 只关心地址匹配当前块地址的事件.
        RDHIT
            CPU 的读取的地址在当前 cache 块中
        RDMISS
            cache 发生了读缺失, 决定换入数据到当前块中
        WRHIT
            CPU 的写地址在当前 cache 块中
        WRMISS
            写缺失, 换入数据到当前块中
        BUSRD
            总线上 (广播的) 信号, 内容是 "某个 CPU 读某个地址"
        BUSWR
            总线上 (广播的) 信号, 内容是 "某个 CPU 写某个地址"
    考虑写无效, 写分配, 写回的策略
    则有以下的状态转移表. 其中事件的地址都是这个 cache 块.
    动作中我们不叙述 cache (如有更新则最后的) 读写操作, 因为它无论如何都会发生

    +------+--------+------------------------------------------+--------+
    | 状态 | 事件   | 动作                                     | 转移   |
    |      |        |                                          |        |
    | INV  | RDHIT  | 不可能                                   | 不可能 |
    | INV  | RDMISS | 调入数据到 cache, 广播 BUSRD             | SHR    |
    | INV  | WRHIT  | 不可能                                   | 不可能 |
    | INV  | WRMISS | 调入数据到 cache, 广播 BUSWR             | MOD    |
    | INV  | BUSRD  | nop                                      | INV    |
    | INV  | BUSWR  | nop                                      | INV    |
    |      |        |                                          |        |
    | SHR  | RDHIT  | nop                                      | SHR    |
    | SHR  | RDMISS | 调入数据到 cache, 广播 BUSRD             | SHR    |
    | SHR  | WRHIT  | 广播 BUSWR                               | MOD    |
    | SHR  | WRMISS | 广播 BUSWR                               | MOD    |
    | SHR  | BUSRD  | nop                                      | SHR    |
    | SHR  | BUSWR  | nop                                      | INV    |
    |      |        |                                          |        |
    | MOD  | RDHIT  | nop                                      | MOD    |
    | MOD  | RDMISS | 块写回内存; 调入数据到 cache; 广播 BUSRD | SHR    |
    | MOD  | WRHIT  | nop                                      | MOD    |
    | MOD  | WRMISS | 块写回内存; 调入数据到 cache; 广播 BUSWR | MOD    |
    | MOD  | BUSRD  | 块写回内存                               | SHR    |
    | MOD  | BUSWR  | 块写回内存                               | INV    |
    +------+--------+------------------------------------------+--------+

目录方法
    相对总线监听, scalability 更好
    采用集中式的目录管理, 对于每一个在某个 cache 中出现的块,
        目录中追踪有哪些 cache 包含这个块
    角色
        考虑一种情形: 某个 CPU P 访问某地址 A 的数据, 缓存中没有,
            于是 [P的缓存] 向保有目录的中心结点请求,
            中心结点根据 A 确定数据是应该从
            另外一个 CPU P2 [的缓存] 中取得, 还是应该从内存中取得
        CACHE:
            LOCAL:
                最初请求数据的 CPU P [的缓存]
            REMOTE:
                另外持有数据的 CPU P2 [的缓存]
        HOME:
            保有目录的中心结点
    块状态
        INVL, SHR, MOD
    事件
        只在 LOCAL:
            RDHIT: 读地址 A, 命中
            RDMISS: 读地址 A, 不命中
            WRHIT: 写地址 A, 命中
            WRMISS: 写地址 A, 不命中
        CACHE -> HOME
            RD: 读 A 地址, 需要目录结点查询并且回送 A 的数据
            WR: 写 A 地址, 需要目录回送 (写回)
            RDWB: 将该 cache 块写回内存, 同时读地址 A 的内容
            WRWB: 将该 cache 块写回内存, 同时写地址 A 的内容
        HOME -> CACHE
            FETCH: 读取某位置 A 的块. 保证 A 在该 cache 中,
                    需要该 cache 回送数据
            INVL: 某位置 A 的块无效化
            FETCHINVL: 两个都做
    状态表
        表中没有写 DATA, 因为 DATA 不可能孤立发生
        CACHE:
            | 状态 | 事件      | 动作                                    | 新状态 |
            |      |           |                                         |        |
            | INVL | RDMISS    | 发送 RD 给 HOME, 接受 DATA, 更新 cache  | SHR    |
            | INVL | RDHIT     | 不可能                                  | 不可能 |
            | INVL | WRMISS    | 发送 WR 给 HOME, 接受 DATA, 更新 cache  | MOD    |
            | INVL | WRHIT     | 不可能                                  | 不可能 |
            | INVL | FETCH     | 不可能                                  | 不可能 |
            | INVL | INVL      | 不可能                                  | 不可能 |
            | INVL | FETCHINVL | 不可能                                  | 不可能 |
            |      |           |                                         |        |
            | SHR  | RDMISS    | 发送 RD 给 HOME, 接受 DATA, 更新 cache  | SHR    |
            | SHR  | RDHIT     | nop                                     | SHR    |
            | SHR  | WRMISS    | 发送 WR 给 HOME, 接受 DATA, 更新 cache  | MOD    |
            | SHR  | WRHIT     | 发送 WR 给 HOME, 接受 DATA, 更新 cache  | MOD    |
            | SHR  | FETCH     | 读数据之后回送 DATA                     | SHR    |
            | SHR  | INVL      | nop                                     | INVL   |
            | SHR  | FETCHINVL | 读数据之后回送 DATA                     | INVL   |
            |      |           |                                         |        |
            | MOD  | RDMISS    | 发送 RDWB 到 HOME, 接受 DATA 更新 cache | SHR    |
            | MOD  | RDHIT     | nop                                     | MOD    |
            | MOD  | WRMISS    | 发送 WRWB 到 HOME, 接受 DATA 更新 cache | MOD    |
            | MOD  | WRHIT     | nop                                     | MOD    |
            | MOD  | FETCH     | 读数据后回送 DATA                       | SHR    |
            | MOD  | INVL      | 不允许                                  | 不允许 |
            | MOD  | FETCHINVL | 读数据, 回送数据和 WB 到 HOME           | INVL   |
        HOME:
            | 共享者      | 事件 | 动作                                                          | 新共享者       |
            |             |      |                                                               |                |
            | 空          | RD   | 从内存中读取数据, 发送                                        | {P}            |
            | 空          | WR   | 从内存中读取数据, 发送                                        | {P}            |
            | 空          | RDWB | 不可能                                                        | 不可能         |
            | 空          | WRWB | 不可能                                                        | 不可能         |
            |             |      |                                                               |                |
            | {P1}        | RD   | 发送 FETCH 给 P1, 接受 DATA 后转发给 P                        | {P1, P}        |
            | {P1}        | WR   | 发送 TODO
            | {P1}        | RDWB | 发送 FETCH 给 P1, 接受 DATA 后转发给 P                        | {P1, P}        |
            | {P1}        | WRWB | 发送 FETCHINVL 给 P1, 接受 DATA 后转发给 P                    | {P}            |
            |             |      |                                                               |                |
            | {P1 ... Pk} | RD   | 发送 FETCH 给任何一个 Pi, 接受 DATA 转发给 P                  | {P, P1 ... Pk} |
            | {P1 ... Pk} | WR   | 发送 INVL 给所有 Pi, 其中有一个 FETCHINVL, 接受 DATA 转发给 P | {P}            |
            | {P1 ... Pk} | RDWB | 不可能                                                        | 不可能         |
            | {P1 ... Pk} | WRWB | 不可能                                                        | 不可能         |

------------------------------------------------------------------------------
同步机制

基本概念
    竞争 racing
    互斥访问
    临界区
    锁
    spinlock
    mutex

常见硬件原语
    swap 或者 xchg
        原子地交换寄存器和内存的值
        init:
            sw zr, 0(&lock)
        lock:
            addiu r1, zr, 1
            swap r1, 0(&lock)
            bne r1 zr -2
        unlock:
            sw zr, 0(&lock)
    test and set
        原子地修改值, 并且返回修改前的值
        init:
            sw zr, 0(&lock)
        lock:
            addiu r1, zr, 1
            tss r1, 0(&lock)
            bne r1, zr, -2
        unlock:
            sw zr, 0(&lock)
    compare and swap
        原子的 cmove
        cas r1, r2, addr:
            { rv = *addr; if *addr == r2 then *addr = r1; return rv; }
        init, unlock:
            sw zr, 0(&lock)
        lock:
            addiu r1, zr, 1
            bne `cas r1, zr, 0(&lock)`, zr, -2
    fetch and add:
        原子地 load 然后将 load 得到值加 1 写入存储
    load linked & store conditional:
        ll 相当于 load, 如果对于同一个地址, ll 和 sc 之间其值没有改变
            那么 sc 成功, 否则失败 (这时候就不写内存了)
        lock:
            while (1) {
                while (ll(&lock) == 1) ;
                if (sc(&lock, 1) == SUCCESS) break;
            }
        unlock:
            lock = 0

实际硬件
    x86:
        指令不一定是原子的! 因为被翻译成 micro-ops.
        指令前缀 lock 可以保证指令的原子性, 通过锁定总线来完成
    arm, mips, powerpc:
        使用 ll 和 sc. 基于 ll 和 sc 可实现其他硬件原语.
        ll: 就是 load, 只是特别地装载进一个特殊的 link reg
        sc: 检查上次 ll 地址的值和 link reg 值是否相等
        发生中断, 或者修改了 ll 地址, link reg 清空

锁和 cache 一致性
    锁在内存中, 一般是在任意内存中 -> 可能被 cache
    如果忙等加锁时, 每次都写内存, 产生大量总线消息
    (加入 cache 一致性之后, 写 cache 要发送消息 update / invalidate)
    > 对于会写锁的, 只有锁被释放才尝试加锁
    > ll 和 sc 天然和 cache 一致性相性很好

栅栏
    强制到达该栅栏的进程等待, 直到所有进程都到达该栅栏
    加上 sense reversing: 第一个进入 barrier 的进程需要重设 barrier
        如果还有进程没有从 barrier 出来, 那么那个没出来的进程
        相当于被重新锁上了
    barrier:
        @process local var sense init=1
        sense = not sense
        spinlock_lock(counter_lock) ;
        counter++
        spinlock_unlock(counter) ;
        counter == TOTAL ?
            counter = 0, release = sense:
            while (release != sense) ; ;

------------------------------------------------------------------------------

1. 如下流水线, 求执行 c_i = a_i + b_i (0 <= i <= 9) 的周期数 ( )?
    执行 sum_{i = 0}^9 x_i 的周期数 ( )?
                          +-------------------------------------+
                          |                                     |
                   +------>               3DT                   +------+
                   |      |                                     |      |
                   |      +-------------------------------------+      |
                   |                                                   |
                   |                                                   |
                   |                                                   |
                   |                                                   |
+------------+     |      +-------------------------------------+      |    +---------------+          +---------------+
|            |     |      |                                     |      |    |               |          |               |
|     DT     +------------>               3DT                   +----------->     DT        +---------->     DT        |
|            |     |      |                                     |      |    |               |          |               |
+------------+     |      +-------------------------------------+      |    +---------------+          +---------------+
                   |                                                   |
                   |                                                   |
                   |                                                   |
                   |                                                   |
                   |      +-------------------------------------+      |
                   |      |                                     |      |
                   +------>               3DT                  +-------+
                          |                                     |
                          +-------------------------------------+

    15; 25;
